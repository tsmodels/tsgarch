---
title: "Modelling volatility using tsgarch"
output: 
    bookdown::html_document2:
        base_format: rmarkdown::html_vignette
        number_sections: yes
        code_folding: show
        css:
              - !expr system.file("rmarkdown/templates/html_vignette/resources/vignette.css", package = "rmarkdown")
              - custom.css
bibliography: '`r system.file("REFERENCES.bib", package="tsgarch")`'
csl: journal-of-finance.csl
vignette: >
  %\VignetteIndexEntry{Modelling volatility using tsgarch}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup,echo=FALSE,message=FALSE}
library(tsgarch)
# TODO: replace sum with \limits and check consistency across documents
# TODO: add section bookmarks everywhere
```

# Introduction {#intro}

The Generalized Auto-Regressive Conditional Heteroscedasticity (**GARCH**) model of [@Bollerslev1986] and the numerous extensions which have followed since, is a 
framework for modeling the dynamics of the conditional variance. It has proved particularly popular, particularly among financial market practitioners, but also
in other areas were the arrival of unexpected new information may lead to non-instantaneous decay (*persistence*) and/or asymmetric reaction to good and 
bad news (*news impact*).

Many programming languages have one or more implementations of GARCH, with R having no less than at least 3, including the `garch` function from the [tseries](https://cran.r-project.org/web/packages/tseries/index.html) package, [fGarch](https://cran.r-project.org/web/packages/fGarch/index.html) 
and [rugarch](https://cran.r-project.org/web/packages/rugarch/index.html). A select R package review is provided in [@Hill2019], and a cross language
package review on asymmetric GARCH in [@Charles2019]. The suggestions and constructive feedback in these papers was taken into account when developing this new package, as were some of the user comments over the last 10 years.

The **tsgarch** package is a partial re-implementation of [rugarch](https://cran.r-project.org/web/packages/rugarch/index.html), by the same author, with
key differences summarized below:

- it does not (yet) implement all GARCH models in rugarch. FIGARCH, Multiplicative Component GARCH and Realized GARCH are not currently implemented.
- it does not implement joint ARFIMA-GARCH estimation. The conditional mean equation only allows for a constant. With so many options 
for modelling the conditional mean, many of which are available in the **tsmodels** framework, it was decided to keep this package simpler 
and avoid the joint estimation of both conditional mean and variance dynamics. While the 2 step estimation approach, whereby the 
residuals of the conditional mean are passed to the variance dynamics estimation, may be less efficient for small sized datasets, 
it is expected to be more flexible in what can be achieved. Additionally, the ARCH-in-mean model is no longer available, 
as it was found to have very limited value within the **tsmodels** framework or in this author's experience. A separate 
[tsarma](https://github.com/tsmodels/tsarma) package for ARMA(p,q)-X models is however available.

-  it makes use of automatic differentiation (autodiff) during estimation, via the [TMB](https://cran.r-project.org/web/packages/TMB/index.html) package. This is in line with similar approaches in other models written in the **tsmodels** framework. Using autodiff allows for more confident estimation and more accurate standard errors.
- it fully implements and correctly documents a number of sandwich estimators making use of the [sandwich](https://cran.r-project.org/web/packages/sandwich/index.html) package framework (with methods for `bread` and `estfun` and `meat`/`meat_HAC` functions).
-  it makes use of S3 methods and classes, abandoning the S4 approach of **rugarch**. Additionally, while making use of standard methods from the 
stats package, some of the methods are based on those exported from [tsmethods](https://github.com/tsmodels/tsmethods), consistent with other 
packages in the **tsmodels** framework. The [Appendix](#appendix:functionmap) provides a table with a hierarchical overview of the 
main functions and methods currently implemented in the package.


# GARCH Estimation {#estimation}

The density function, based on the distributions available in the [tsdistributions](https://github.com/tsmodels/tsdistributions)
package, is expressed in terms of the location, scale, skew and shape parameters $\left\{\mu_t,\sigma_t,\zeta,\nu\right\}$, normalized 
to give zero mean and unit variance:

\begin{equation}
z_t = \frac{y_t - \mu_t}{\sigma_t}
(\#eq:1)
\end{equation}

where $\mu_t = E\left(y_t|x_t \right)$ and $\sigma^2_t = E\left((y_t - \mu_t)^2|x_t\right)$, with $x_t$ the information set available
at the period immediately before time $t$ and may include both external regressors and lagged values of $y$. Assuming that the distribution of $z_t$ is independent of the conditioning information set^[In the case when this assumption is relaxed, this will give rise to a non constant conditional distribution. See [@Hansen1994].], then:

\begin{equation}
g\left(z_t|\zeta,\nu\right) = \frac{d}{dz}P\left(z_t<z|\zeta,\nu\right)
(\#eq:2)
\end{equation}

which is related to the distribution of $y_t$ by through the scale term:

\begin{equation}
f\left(y_t|\mu_t,\sigma_t,\zeta,\nu\right) = \frac{1}{\sigma_t}g\left(z_t|\zeta,\nu\right)
(\#eq:3)
\end{equation}

The parameter set $\theta$, which includes the conditional mean, conditional variance and distributional 
skew and shape parameters is estimated by minimizing the negative of the log-likelihood:

\begin{equation}
\theta_{ML} = \underset{\theta}{\operatorname{arg\min}}\sum^T_{t=1}-\mathrm{log}\left(\frac{1}{\sigma_t}g\left(z_t|\zeta,\nu\right)\right)
(\#eq:4)
\end{equation}

subject to upper and lower parameters bound constraints as well as the persistence constraint ($P<1$) . Estimation is performed using
the [nloptr](https://cran.r-project.org/web/packages/nloptr/index.html) solver with analytic gradient and Jacobian of the constraints^[For
some of the problems, the persistence has an easy closed form solution and therefore the Jacobian is hardcoded instead of making use of
autodiff.] 
provided through autodiff in **TMB**.

## Recursion Initialization ($\sigma^\delta_0$) {#estimation:recursion}

Initialization of the GARCH recursion takes the following generalized form:

\begin{equation}
\sigma^\delta_0 = \lambda^T\hat\sigma^\delta + (1-\lambda)\sum_{j=0}^{T-1}\lambda^j |\varepsilon|^\delta_{1+j},\quad \lambda\in\{0,1\}
(\#eq:5)
\end{equation}

where : $\hat\sigma^\delta=\frac{1}{T-1}\sum_{j=1}^{T}|\varepsilon|^\delta_j$

Therefore setting $\lambda=1$ we obtain the sample variance, else values of $\lambda$ less than one and greater than zero yields the exponential smoothing backcast estimator, whereas setting $T$ to some value other than the total length of the residuals $\varepsilon$ and $\lambda=0$ will give the fixed sample estimator which has been somewhat popular (e.g. using only the first n observations). The exponent $\delta$ is for the power ARCH type models, otherwise it is set to 2 (variance).

## Model Persistence, Long Run Variance and Half-Life {#estimation:persistence}

The persistence ($P$) of a GARCH model is a measure which quantifies the degree of volatility clustering and rate of decay. It also forms a bounding constraint ($<1$)
on the model dynamics in order to ensure stationarity. Another way to think about persistence is by looking at the unconditional variance of a GARCH model which is defined as

\begin{equation}
\hat\sigma^2 = \frac{\omega}{1-P}
(\#eq:6)
\end{equation}

which illustrates that positivity of the unconditional variance requires $P<=1$, whilst existence of this value requires $P<1$, which is not the case for the integrated GARCH model where $P=1$ by design. The form that $P$ takes will depend on the type of model, with the formulas provided in Section \@ref(estimation:flavors). Closely related to the persistence is the half-life measure which is defined as the number of days it takes for a shock to revert half way back to the long run variance, and defined as $-\textrm{log}_e(2)/\mathrm{log}_e(P)$. 

A special note is warranted for the half-life of the Component GARCH (`cgarch`) model which is composed of a permanent and transitory component, each of which
have a persistence (see Equation \@ref(eq:42)). The permanent component half-life, based on the estimate of $\rho$, measures the time taken for the long-run influence 
of a shock in volatility to revert by half towards it's long run unconditional value, whereas the transitory component half-life accounts for the time taken for a shockâ€™s influence to revert to its long-run rate.


## Variance Targeting ($\bar\omega$) {#estimation:targeting}

Variance targeting sets the value of the GARCH intercept ($\omega$) to it's long run estimate as :

\begin{equation}
\bar\omega = \hat\sigma^2\left(1 - P\right) - \sum^m_{j=1}\xi_j \hat v_j 
(\#eq:7)
\end{equation}

where $\hat\sigma^2$ is the unconditional variance of $\varepsilon^2$, consistently estimated by its sample counterpart, $P$ is the model persistence and $\hat v$ the mean of the external variance regressors (if present). A review of variance targeting can be found in [@Francq2011]. In this author's experience, more than 90\% of model estimation problems come from trying to estimate $\omega$ as a result of parameter scaling issues. In **tsgarch**, despite attempts to apply some scaling rules during optimization, failure to converge will sometimes happen, in which case the model will be automatically re-estimated with variance targeting (the output will indicate when this has happened).

## External Regressors ($\xi$) {#estimation:regressors}

Inclusion of additive external regressors has always been a little tricky when it comes to variance modelling due to the non-negativity constraint. This is an issue in all GARCH flavors with the exception of the exponential model. One way to deal with this is to constrain coefficients and regressors to be strictly positive which is not ideal. Another option, which is now offered in **tsgarch** is to have multiplicative regressors where the intercept is now calculated as follows:

\begin{equation}
\omega_t = \mathrm{exp}\left(\omega + \sum^m_{j=1}\xi_j v_{j,t}\right)
(\#eq:8)
\end{equation}

which does not require any bound constraints on either the constant $\omega$ or the regressors.

## News Impact Curve {#estimation:newsimpact}

[@Engleng1993] defined the news impact curve as a way to analyze the effect of 
news on the conditional variance by keeping constant the information dated $t-2$ 
and earlier. Therefore, the long run variance of the model is used in place
of $\sigma_{t-1}$ and a range of values for $\varepsilon_t$ are chosen to show
how news of different sign and size impact the current conditional variance.
A most interesting example of this found in the Family GARCH model of [@Hentschel1995] 
which accommodates both shifts and rotations in the news impact curve, with the 
shift factor being the the main source of asymmetry for small shocks, 
and rotation driving larger shocks.

## Standard Errors {#estimation:stderrors}

The **tsgarch** package makes use of the methods available in the [sandwich](https://cran.r-project.org/web/packages/sandwich/index.html) 
package to deliver a number of different estimators for the parameter covariance matrix (S). 

Define the objective function ($\Psi$) as the log-likelihood of the GARCH model with respect to the data and
parameters:

\begin{equation}
\Psi\left(y,x,\theta\right) = \sum^T_{t=1}\mathrm{log}\left(\frac{1}{\sigma_t}g\left(z_t|\zeta,\nu\right)\right)
(\#eq:9)
\end{equation}

where $\frac{1}{\sigma_t}g\left(z_t|\zeta,\nu\right)$ is defined in Section \@ref(estimation). The estimating
(or score) function of the objective function is then:

\begin{equation}
\psi\left(y,x,\theta\right) =  \frac{\partial\Psi\left(y,x,\theta\right)}{\partial\theta}
(\#eq:10)
\end{equation}

Inference about the parameter set $\theta$ relies on a central limit theorem (CLT) with $\sqrt{n}$ consistency:

\begin{equation}
\sqrt{n}\left(\hat\theta - \theta\right) \overset{d}{\to} N\left(0, S(\theta)\right)
(\#eq:10)
\end{equation}

where $\overset{d}{\to}$ indicates convergence in distribution. The **sandwich** package defines the
sandwich estimator $S\left(\theta\right)$ as:

\begin{equation}
S\left(\theta\right) = B\left(\theta\right) M\left(\theta\right) B\left(\theta\right)
(\#eq:11)
\end{equation}

where the meat (**M**) of the sandwich is the variance of the estimating function:

\begin{equation}
M\left(\theta\right) = \textrm{VAR}\left[\psi\left(y,x,\theta\right)\right]
(\#eq:12)
\end{equation}

and the bread (**B**) is the inverse of the expectation of its first derivative ($\psi^{'}$):

\begin{equation}
B\left(\theta\right) = \left(E\left[-\psi^{'}\left(y,x,\theta\right)\right]\right)^{-1}
(\#eq:13)
\end{equation}

In **tsgarch**, the following 4 estimators for the covariance matrix are defined:

### Direct (**H**)

\begin{equation}
S\left(\theta\right) = B\left(\theta\right) = -H^{-1} 
(\#eq:14)
\end{equation}

This makes use of the analytic hessian ($H$) at the optimal solution.

### Outer product of the gradient (**OP**)
\begin{equation}
\begin{aligned}
S\left(\theta\right) &= M\left(\theta\right) \\
&= \frac{1}{T}\sum^T_{t=1}\psi\left(y_t,x_t,\hat\theta\right)\psi\left(y_t,x_t,\hat\theta\right)^{'}
\end{aligned}
(\#eq:15)
\end{equation}

The estimating function is essentially the Jacobian of the likelihood at each time step with respect to the parameters.
Currently, this is based on the `jacobian` function from [numDeriv](https://cran.r-project.org/web/packages/numDeriv/index.html), 
but will be replaced in a future version by the analytic solution from TMB.

### Quasi-Maximum Likelihood (**QML**)

\begin{equation}
\begin{aligned}
S\left(\theta\right) &=  B\left(\theta\right) M\left(\theta\right) B\left(\theta\right) \\
& = H^{-1}\left(\psi\left(y_t,x_t,\hat\theta\right)\psi\left(y_t,x_t,\hat\theta\right)^{'}\right)H^{-1}
\end{aligned}
(\#eq:16)
\end{equation}

### HAC

In the presence of residual autocorrelation, the HAC estimator is based on the weighted
empirical autocorrelations of the empirical estimating functions:

\begin{equation}
\begin{aligned}
M_{HAC} & = \frac{1}{T}\sum^T_{i,j=1}w_{|i-j|}\psi\left(y_t,x_t,\hat\theta\right)\psi\left(y_t,x_t,\hat\theta\right)^{'} \\
S\left(\theta\right) &=  B\left(\theta\right) M_{HAC}\left(\theta\right) B\left(\theta\right)
\end{aligned}
(\#eq:17)
\end{equation}

where the weights $w$ can either be fixed or estimated using an appropriate choice of strategies. In the **tsgarch**
package, the bandwidth of [@Newey1994] is used to automatically calculate the lag and then the weights, based on the
`bwNeweyWest` function from the **sandwich** package.

## Parameter Scaling {#estimation:scaling}

The estimation strategy involves 2 passes through the optimizer. During the first pass, 
the parameters are first estimated using no-scaling. In the second pass, the problem
is reformulated based on re-scaling the n parameters and their bounds by the vector 
$s = \sqrt{\{s^{-1}_{1,1},s^{-1}_{2,2},\ldots,s^{-1}_{n,n}\}}$, where $s_{i,i}$ are the
diagonals of the hessian at the previous solution.

The rescaled parameters and their bounds are passed to the optimizer, whilst the
underlying C++ TMB code scales them back in order to perform the likelihood calculations
and correctly adjust the analytic derivatives in the presence of this scaling vector.

The reason for performing this extra step, is to avoid some bad solutions as a result
of large differences in scaling (particularly with respect to the constant $\omega$), 
and to avoid issues with the derivatives. The optimal hessian and scores (the estimating
function) in the output object are based on the scaled versions from the second pass which
are then re-scaled back.  This approach was found to offer substantial stability at a low
cost since the second pass will usually only require a few additional iterations. Some
justification for this method can be found in [@Yang2010], where the current implementation
is similar to that proposed by [@Rao2019].


## GARCH Flavors {#estimation:flavors}

The **tsgarch** package implements a number of different flavors of GARCH, including
the option of 10 different distributions. The next subsections provide details 
of each model's formulation. For each model, the dynamics of the conditional mean take
the following form:

\begin{equation}
\begin{aligned}
\mu_t &= \mu \\
y_t &= \mu_t + \varepsilon_t,\quad \varepsilon_t\sim D\left(0, \sigma_t,\zeta,\nu, \lambda\right)
\end{aligned}
(\#eq:18)
\end{equation}

where D is one of the available distributions from **tsdistributions**, $\zeta$ the skew parameter,
$\nu$ the shape parameter and $\lambda$ an additional shape parameter used in the Generalized Hyperbolic
distribution. These additional distributional parameters may be present in some combination or not
at all depending on the distribution. It is also possible to set $\mu$ to zero and pass a series
of pre-filtered residual values from some other model as discussed in Section \@ref(intro).

Note that the parameter symbols for the model equations presented in the following subsections 
will be exactly the same as those output by the package, with the `summary` method on estimated
objects having the option to replace the names of parameters with symbols when transforming to 
a **flextable** object (`as_flextable` method on summary object).

### Vanilla GARCH (`garch`) {#garch}
The vanilla GARCH model of [@Bollerslev1986] extended the ARCH model of [@Engle1982] to include a
moving average term making it more closely aligned with the literature on ARMA processes, and allowing
for a wider range of behavior and more persistent volatility.

#### Equation

\begin{equation}
\begin{aligned}
\omega_t &= \omega + \sum^{m}_{j=1}\xi_j v_{j,t}\\
\sigma^2_t &= \omega_t + \sum^q_{j=1}\alpha_j\varepsilon^2_{t-j} + \sum^p_{j=1}\beta_j\sigma^2_{t-j}
\end{aligned}
(\#eq:19)
\end{equation}

#### Persistence

\begin{equation}
\begin{aligned}
P &= \sum^q_{j=1}\alpha_j + \sum^p_{j=1}\beta_j
\end{aligned}
(\#eq:20)
\end{equation}

#### News Impact

\begin{equation}
\begin{aligned}
\sigma^2_{i} &= \omega + \alpha \varepsilon^2_i + \beta\bar\sigma^2
\end{aligned}
(\#eq:21)
\end{equation}

where $\bar\sigma^2  = \frac{\omega}{1 - P}$ and represents the long run 
unconditional variance.

#### Model Constraints

 * $\alpha_j>0$
 * $\beta_j>0$
 * $\omega>0$
 * $P < 1$


### Integrated GARCH (`igarch`) {#igarch}

The integrated GARCH model of [@Engle1986] assumes that the
persistence $P = 1$, hence shocks are permanent and the unconditional 
variance infinite. The motivation behind this model was to capture
the long memory behavior observed in some financial time series.^[Though many times 
it is also possible that this is the result of omitted structural breaks]. However,
[@Nelson1990] showed that the IGARCH process with no drift ($\omega=0$) would
converge to zero with probability one. In the presence of a drift term ($\omega>0$)
the process is neither covariance stationary nor does it have well-defined 
unconditional variance, though it still remains strictly stationary and 
ergodic. For trully long memory processes, other GARCH models should be considered
such as the Fractionally Integrated GARCH (FIGARCH) or Hyperbolic GARCH (HYGARCH) which
are planned for inclusion at a later time.

#### Equation

\begin{equation}
\begin{aligned}
\omega_t &= \omega + \sum^{m}_{j=1}\xi_j v_{j,t}\\
\sigma^2_t &= \omega_t + \sum^q_{j=1}\alpha_j\varepsilon^2_{t-j} + \sum^p_{j=1}\beta_j\sigma^2_{t-j}
\end{aligned}
(\#eq:22)
\end{equation}

#### Persistence
The persistence in the `igarch` model is set to 1 and forms a binding constraint on the
parameters.

\begin{equation}
\begin{aligned}
P = \sum^q_{j=1}\alpha_j + \sum^p_{j=1}\beta_j = 1
\end{aligned}
(\#eq:23)
\end{equation}

#### News Impact

Not defined.

#### Model Constraints

 * $\alpha_j>0$
 * $\beta_j>0$
 * $\omega>0$
 * $P = 1$


### Exponentially Weighted Moving Average (`ewma`)

The Exponentially Weighted Moving Average (`ewma`) model is a restricted `igarch`
model where the drift term ($\omega$) is set to zero. It has proven popular 
among some practitioners for it's simplicity and speed, with coefficients
most often hard coded rather than estimated based on prior knowledge. However,
as mentioned in Section \@ref(igarch) the variance will converge to zero in a
finite number of steps so it is unlikely to be a good model for anything but very
short term forecasting.

#### Equation

The `ewma` equation is usually written as $\sigma^2_t = \left(1 - \lambda\right)\varepsilon^2_{t-1} + \lambda\sigma^2_{t-1}$,
but we present below the more general model which shows that it is a restricted `igarch` model with no drift (although
it is always possible to include regressors).

\begin{equation}
\begin{aligned}
\omega_t &= \sum^{m}_{j=1}\xi_j v_{j,t}\\
\sigma^2_t &= \omega_t + \sum^q_{j=1}\alpha_j\varepsilon^2_{t-j} + \sum^p_{j=1}\beta_j\sigma^2_{t-j}
\end{aligned}
(\#eq:24)
\end{equation}

#### Persistence
Since this is simply a restricted `igarch` model, the persistence is set to 1 
and forms a binding constraint on the parameters as in Equation \@ref(eq:23).

#### News Impact
Not defined.


### Exponential GARCH (`egarch`) {#egarch}
The exponential GARCH model of [@Nelson1991] allows for asymmetric effects between 
positive and negative returns, and does not require specific parameter restrictions
to ensure positivity of the variance since the modelling is performed on the 
log variance.

#### Equation
\begin{equation}
\begin{aligned}
\omega_t &= \omega + \sum^{m}_{j=1}\xi_j v_{j,t}\\
{\log_e}\left(\sigma^2_t\right) &= \omega_t + \sum\limits_{j = 1}^q \left(\alpha_j z_{t - j} + \gamma_j\left(\left|z_{t - j}\right| - E\left|z_{t - j}\right|\right)\right) + \sum\limits_{j = 1}^p \beta_j\log_e\left(\sigma^2_{t - j}\right)
\end{aligned}
(\#eq:25)
\end{equation}

where $z_t = \frac{\varepsilon_t}{\sigma_t}$, with expectation of the absolute moment given by:

\begin{equation}
E\left|z_{t - j}\right| = \int\limits_{-\infty}^\infty{\left|z\right|} D\left(z, 0,1,\zeta,\nu,\lambda\right) dz
(\#eq:26)
\end{equation}

For symmetric distributions, the absolute moment is usually available in closed form (such as in the Gaussian
case where it is $\sqrt{\frac{2}{\pi}}$). For other distributions this is calculated using Gauss-Kronrod 
quadrature in the C++ TMB code so that it is forms part of the autodiff tape.


#### Persistence

The persistence has a rather simple form based on the sum of the moving average coefficients.

\begin{equation}
P = \sum\limits_{j = 1}^p \beta_j
(\#eq:27)
\end{equation}

#### News Impact

\begin{equation}
\sigma^2_i = {\exp}\left(\omega + \alpha z_i + \gamma \left(\left|z_i\right| - E\left|z_i\right|\right) + \beta {\log_e}\left(\sigma^2\right)\right)
(\#eq:28)
\end{equation}

where $\bar\sigma^2 = \exp\left(\frac{\omega}{1 - P}\right)$, represents the long run unconditional variance.

#### Model Constraints

 * $P < 1$

### GJR GARCH (`gjrgarch`)

The GJR GARCH model of [@Glosten1993] models positive and negative shocks on
the conditional variance asymmetrically using a leverage term for past squared,
negative innovations via the indicator function $I$. 

#### Equation
\begin{equation}
\begin{aligned}
\omega_t &= \omega + \sum^{m}_{j=1}\xi_j v_{j,t}\\
\sigma^2_t & = \omega_t + \sum\limits_{j = 1}^q \left(\alpha_j\varepsilon^2_{t - j} + \gamma_j {I}_{[\varepsilon_{t-j}\le 0]}\varepsilon^2_{t - j}\right) + \sum\limits_{j = 1}^p \beta_j\sigma^2_{t-j}
\end{aligned}
(\#eq:28)
\end{equation}

where $\gamma_j$ now represents the *leverage* term. The indicator function $I$
takes on value 1 for $\varepsilon \le 0$ and 0 otherwise. Because of the
presence of the indicator function, the persistence of the model now crucially
depends on the asymmetry of the conditional distribution.

#### Persistence

\begin{equation}
\begin{aligned}
P &= \sum\limits_{j = 1}^q \alpha_j  + \sum\limits_{j = 1}^p \beta_j + \sum\limits_{j = 1}^q \gamma _j\kappa
\end{aligned}
(\#eq:29)
\end{equation}

where $\kappa$ is the expected value of $\varepsilon_t \le 0$. Since this represents
the probability of being less than zero, we can work directly with the standardized
innovations $z_t$:

\begin{equation}
\begin{aligned}
\kappa &= E\left[I_{[z_{t-j}\le 0]}\right] \\
&= \int\limits_{-\infty}^0 D\left(z, 0,1,\zeta,\nu,\lambda\right) dz
\end{aligned}
(\#eq:30)
\end{equation}

For symmetric distributions this value is always 0.5, 
but for skewed distributions this is calculated using Gauss-Kronrod quadrature in 
the C++ TMB code so that it is forms part of the autodiff tape allowing to also 
extract the Jacobian of this function for use with the inequality constraint
in the **nloptr** solver.

#### News Impact

\begin{equation}
\sigma^2_i = \omega + \alpha \varepsilon^2_i + \gamma {I}_{[\varepsilon_i\le 0]}\varepsilon^2_i + \beta \bar\sigma^2
(\#eq:31)
\end{equation}

where $\bar\sigma^2 = \frac{\omega}{1 - P}$, represents the long run unconditional variance.


#### Model Constraints

 * $\alpha_j>0$
 * $\beta_j>0$
 * $\gamma_j>0$
 * $\omega>0$
 * $P < 1$


### Asymmetric Power ARCH (`aparch`)

The asymmetric power ARCH model of [@Ding1993] allows for both leverage and
the Taylor effect, named after [@Taylor1986] who observed that the sample
autocorrelation of absolute returns was usually larger than that of squared
returns.

#### Equation
\begin{equation}
\begin{aligned}
\omega_t &= \omega + \sum^{m}_{j=1}\xi_j v_{j,t}\\
\sigma^{\delta}_t  &= \omega_t + \sum\limits_{j = 1}^q \alpha_j\left(\left|\varepsilon_{t - j}\right| - \gamma_j\varepsilon_{t - j}\right)^{\delta} + \sum\limits_{j = 1}^p \beta_j\sigma^{\delta}_{t - j}
\end{aligned}
(\#eq:32)
\end{equation}

where $\delta  \in {\mathbb{R}^ + }$, being a Box-Cox transformation of $\sigma_t$,
and $\gamma_j$ the coefficient in the leverage term.

#### Persistence

\begin{equation}
\begin{aligned}
P = \sum\limits_{j = 1}^p \beta_j  + \sum\limits_{j = 1}^q \alpha_j \kappa_j
\end{aligned}
(\#eq:33)
\end{equation}

where $\kappa_j$ is the expected value of the standardized residuals $z_t$ under
the Box-Cox transformation of the term which includes the leverage coefficient:

\begin{equation}
\begin{aligned}
\kappa_j  & = E\left(\left|z_{t-j}\right| - \gamma_j z_{t-j}\right)^\delta \\
&= \int\limits_{- \infty}^\infty \left(\left|z\right| - \gamma_j z\right)^{\delta} D\left(z, 0,1,\zeta,\nu,\lambda\right) dz
\end{aligned}
(\#eq:34)
\end{equation}

For symmetric distributions there are closed form solutions for this expression, 
but for skewed distributions this is calculated using Gauss-Kronrod quadrature in 
the C++ TMB code so that it is forms part of the autodiff tape allowing to also 
extract the Jacobian of this function for use with the inequality constraint
in the **nloptr** solver.

#### News Impact

\begin{equation}
\sigma^2_i = \left(\omega + \alpha\left(\left|\varepsilon_i\right| - \gamma\varepsilon_i\right)^{\delta}  + \beta\bar\sigma^{\delta}\right)^{\frac{2}{\delta}}
(\#eq:35)
\end{equation}

where $\bar\sigma^{\delta} = \frac{\omega}{1 - P}$, represents the long run unconditional volatility
raised to the power of $\delta$.

#### Model Constraints

 * $\alpha_j>0$
 * $\beta_j>0$
 * $|\gamma_j|<1$
 * $\delta>0$
 * $\omega>0$
 * $P < 1$


### Family GARCH (`fgarch`)

The family GARCH model of [@Hentschel1995] is a large omnibus model which
subsumes some of the most popular GARCH models. It allows for both shifts 
and rotations in the news impact curve, where the shift is the main source 
of asymmetry for small shocks while rotation drives larger shocks. The following
restrictions in the parameters leads to specific models:

* GARCH: $\delta = 2$, $\eta_j = \gamma_j = 0$
* Absolute Value GARCH: $\delta = 1$, $|\gamma_j|\le1$
* GJR GARCH: $\delta=2$,$\eta_j=0$
* Threshold GARCH: $\delta=1$,$\eta_j=0$,$|\gamma_j|\le1$
* Nonlinear GARCH: $\gamma_j = \eta_j = 0$
* Nonlinear Asymmetric GARCH: $\delta=2$,$\gamma_j=0$
* APARCH: $\eta_j=0$, $|\gamma_j|\le1$

#### Equation
\begin{equation}
\begin{aligned}
\omega_t &= \omega + \sum^{m}_{j=1}\xi_j v_{j,t}\\
\sigma^{\delta}_t & = \omega_t + \sum\limits_{j = 1}^q \alpha_j\sigma^{\delta}_{t - j}\left(\left|z_{t - j} - \eta_j\right| - 
\gamma_j\left(z_{t - j} - \eta_j\right)\right)^{\delta} + \sum\limits_{j = 1}^p \beta_j\sigma^{\delta}_{t - j}
\end{aligned}
(\#eq:36)
\end{equation}

#### Persistence

\begin{equation}
\begin{aligned}
P = \sum\limits_{j = 1}^p \beta_j + \sum\limits_{j = 1}^q \alpha_j\kappa_j
\end{aligned}
(\#eq:37)
\end{equation}

where $\kappa_j$ is the expected value of the standardized residuals $z_t$ under
the Box-Cox transformation of the absolute value asymmetry term

\begin{equation}
\begin{aligned}
\kappa_j &= E\left(\left|z_{t - j} - \eta_j\right| - \gamma_j\left(z_{t - j} - \eta_j\right)\right)^{\delta} \\
& = \int\limits_{-\infty}^\infty \left(\left|z - \eta_j\right| - \gamma_j\left(z -\eta _j\right)\right)^{\delta} D\left(z, 0,1,\zeta,\nu,\lambda\right) dz
\end{aligned}
(\#eq:38)
\end{equation}

There are no simple closed form solutions for this expression so it is calculated 
using Gauss-Kronrod quadrature in the C++ TMB code so that it is forms part of 
the autodiff tape allowing to also extract the Jacobian of this function for use 
with the inequality constraint in the **nloptr** solver.

#### News Impact

\begin{equation}
\sigma^2_i = \left(\omega + \alpha\bar\sigma^{\delta}\left(\left|z_i - \eta\right| - \gamma\left(z_i - \eta\right)\right)^{\delta}  + 
\beta \bar\sigma^{\delta}\right)^{\frac{2}{\delta}}
(\#eq:39)
\end{equation}

where $\bar\sigma^{\delta} = \frac{\omega}{1 - P}$, represents the long run unconditional volatility
raised to the power of $\delta$.

#### Model Constraints

 * $\alpha_j>0$
 * $\beta_j>0$
 * $|\gamma_j|<1$
 * $|\eta_j|<10$
 * $\delta>0$
 * $\omega>0$
 * $P < 1$


### Component GARCH (`cgarch`)
The model of [@Lee1999] decomposes the conditional variance into a permanent and 
transitory component so as to investigate the long- and short-run movements of 
volatility affecting securities. 

#### Equation

\begin{equation}
\begin{aligned}
\omega_t &= \omega + \sum^{m}_{j=1}\xi_j v_{j,t} + \rho\omega_{t-1} + \phi\left(\varepsilon^2_{t-1}-\sigma^2_{t-1}\right)\\
\sigma^2_t &= \omega_t + \sum^q_{j=1}\alpha_j\left(\varepsilon^2_{t-j} - \omega_{t-j}\right) + 
\sum^p_{j=1}\beta_j\left(\sigma^2_{t-j} - \omega_{t-j}\right)
\end{aligned}
(\#eq:40)
\end{equation}

The process can be re-written in an alternative form to better highlight the
decomposition of the permanent and transitory components, shown below for the
Component GARCH(1,1) model:

\begin{equation}
\begin{aligned}
&\textrm{Permanent: } &\omega_t &= \omega + \sum^{m}_{j=1}\xi_j v_{j,t} + \rho\omega_{t-1} + \phi\left(\varepsilon^2_{t-1}-\sigma^2_{t-1}\right) \\
&\textrm{Transitory: } &s^2_t &= \left(\alpha + \beta\right) s^2_{t-1} + \alpha\left(\varepsilon^2_{t-1} - \sigma^2_{t-1}\right) \\
&\textrm{Total: } &\sigma^2_t &= \omega_t + s^2_t
\end{aligned}
(\#eq:41)
\end{equation}

The parameters $\alpha$ and $\phi$ correspond to immediate impacts of volatility
shocks $\left(\varepsilon^2_{t-j} - \sigma^2_{t-j}\right)$ on the transitory and 
permanent components, whereas $\left(\alpha + \beta\right)$ and $\rho$ 
measure the persistence of the transitory and permanent components, respectively.

#### Persistence

\begin{equation}
\begin{aligned}
&\textrm{Transitory: } & P^T &= \sum^q_{j=1}\alpha_j + \sum^p_{j=1}\beta_j\\
&\textrm{Permanent: } & P^P &= \rho
\end{aligned}
(\#eq:42)
\end{equation}

#### News Impact

Since the component GARCH model can be represented as a restricted GARCH(2,2)
model, we derive the news impact curve using this representation to arrive at
the following equation:

\begin{equation}
\sigma^2_t = \omega + \rho\bar\sigma^2 + \phi\left(\varepsilon^2_t - \bar\sigma^2\right)+\alpha\left(\varepsilon^2_t - \omega\right) + \beta\left(\bar\sigma^2 - \omega\right)
(\#eq:43)
\end{equation}

where the unconditional variance $\bar\sigma^2 = \frac{\omega}{1-\rho}$.

#### Constraints

 * $\alpha_j>0$
 * $\beta_j>0$
 * $\phi>0$
 * $\rho>0$
 * $\omega>0$
 * $1>P^P>P^T>0$
 * $\beta>\phi$
 * $\alpha>\phi$

# Demonstration {#demo}

## Estimation {#demo:estimation}

We use log returns of the adjusted closing price of the S\&P500 ETF dataset from the 
[tsdatasets](https://github.com/tsmodels/tsdatasets) package
for the demonstration, which spans the period 1993-01-29 / 2023-03-30.

```{r,warning=FALSE,message=FALSE,cache=T}
library(xts)
y <- tsdatasets::spy
r <- diff(log(y))[-1]
spec <- garch_modelspec(r, model = "aparch", constant = TRUE, order = c(1,1),
                        distribution = "jsu", variance_targeting = FALSE)
mod <- estimate(spec)
print(summary(mod))
```

Notice that the persistence is also included in the summary report, with standard errors,
and calculated based on the `sdreport` function from **TMB** which performs
these calculations on any linear or nonlinear operation on parameters in a user template
through the `ADREPORT()` macro.


A nicer table can be generated using the `as_flextable` method, which we illustrate
below and this time we generate QMLE standard errors:

```{r}
s <- summary(mod, vcov_type = "QMLE")
as_flextable(s, include.symbols = TRUE, include.equation = TRUE, table.caption = "SPY - APARCH(1,1) - JSU")
```

The plot method on the estimated object provides a visual summary of the volatility
against the absolute returns together with a news impact plot and a Q-Q plot of
the GARCH standardized residuals against the theoretical samples from the distribution
used. Notice how the news-impact curve is highly asymmetric, with only negative shocks
having any noticeable impact on the variance.

```{r,fig.width=8,fig.height=6}
plot(mod)
```


## Prediction {#demo:prediction}

The `predict` method will generate both a forecast of the volatility as well
as generate a sample predictive distribution of the residuals,  
parametrically from the model distribution or using the bootstrap of [@Pascual2006]. 
This can then be injected into the predict method of other models in the **tsmodels** 
framework. The returned object is of class `tsmodel.predict` for which a 
plot method is available in the **tsmethods** package.

The following plot shows the predicted volatility for the next 150 periods. Using the
`halflife` method, we find that this is equal to about `r ceiling(halflife(mod))` 
periods. The intersection of the vertical and horizontal dotted lines in the plot
show that this is indeed the case with the volatility recovering 50\%
of the way back to its long run value of `r round(sqrt(unconditional(mod)),3)`.


```{r,fig.width=7,fig.height=5}
p <- predict(mod, h = 150, bootstrap = TRUE, nsim = 5000)
hl <- ceiling(halflife(mod))
long_run <- sqrt(unconditional(mod))
par(mar = c(3,2,3,3))
plot(as.zoo(p$sigma), ylab = expression(~sigma[t+h]), xlab = "", main = "T+150 Forecast", cex.main = 0.8)
abline(h = as.numeric(p$sigma[hl]), lty = 2, col = "grey5")
abline(v = index(p$sigma[hl]), lty = 2, col = "grey5")
abline(h = long_run, lty = 1, col = "tomato")
grid()
```

Given that we used the bootstrap method, we can also plot the distribution
of the conditional volatility which is of class `tsmodel.distribution` for which
plot methods are exported from the **tsmethods** package.

The bootstrap method is based on re-sampling standardized residuals from the 
empirical distribution of the fitted model to generate future realizations of 
the series and sigma. There are two ways to do this: one takes into account 
parameter uncertainty by building a simulated distribution of the parameters 
through simulation and refitting, and one which only considers distributional 
uncertainty and hence avoids the expensive and lengthy parameter distribution 
estimation. In the latter case, prediction intervals for the 1-ahead sigma 
forecast will not be available since only the parameter uncertainty is 
relevant in GARCH type models in this case. In **tsgarch** only the latter method
is currently implemented (unlike in **rugarch**).


```{r,fig.width=7,fig.height=5}
par(mar = c(3,2,3,3))
plot(p$sigma_sim, median_color = NA, gradient_color = 'snow3', interval_color = "tomato", main = "T+150 Volatility Forecast Distribution")
lines(index(p$sigma), as.numeric(p$sigma), col = 'steelblue')
```

Finally, we can just plot the predicted object which contains the simulated
predictive density of the model.

```{r,fig.width=7,fig.height=5}
par(mar = c(3,2,3,3))
plot(p, n_original = 200, interval_quantile = c(0.01, 0.99), median_color = 'green', gradient_color = 'snow3', interval_color = "orange", main = "T+150 Forecast Distribution")
lines(index(p$mean), as.numeric(p$mean), col = 'steelblue')
```

## Backtesting {#demo:backtesting}

The `tsbacktest` method allows the generation of rolling forecasts using an expanding
window and returns the values needed to describe the forecast conditional density including
the mean, sigma, skew, shape and lambda values, which may be of particular interest for
evaluating value at risk or other distributional measures.

The backtest method has 2 key arguments, the forecast horizon (h) and the number
of period before a model is re-estimated to generate a fresh set of parameters from
which to forecast. In the case of rolling forecasts, the data is filtered (using the
`tsfilter` method) every period in order to update the last state from which new 
forecasts of length h are generated. Forecasts are *uniquely defined* by their horizon
and and forecast date.

The `tsbacktest` method across all packages in the **tsmodels** framework allows the
use of parallel processing via the [future](https://cran.r-project.org/web/packages/future/index.html) 
package.

```{r,echo=FALSE,eval=T}
b <- readRDS("~/github/tsgarch_testing/backtest.rds")
```

```{r, warning=FALSE,message=FALSE,eval=F}
# create a mew specification
spec <- garch_modelspec(r, model = "egarch", constant = TRUE, distribution = "jsu")
# set up resources
library(future)
plan("future::multisession", workers = 10)
b <- tsbacktest(spec, start = 7000, h = 10, estimate_every = 30, rolling = TRUE, trace = FALSE)
# return to sequential
plan("future::sequential")
```

The main component of the returned list is a table with the following headings:

```{r}
print(b$table, topn = 2, trunc.cols = T)
```

The `estimation_date` shows the date on which the model was estimated from which
the parameters were used for the forecast. It also outputs a convergence code which
returns 1 if both the Kuhn-Karush-Tucker conditions (gradient and positive definite Hessian)
were satisfied^[This is based on the `kktchk` function from the [optimx](https://cran.r-project.org/web/packages/optimx/index.html) 
package.], else 0 which may mean that one or both conditions were not satisfied.
The filter date is $t_0$ date on which the forecast was made, with `horizon` denoting
the n-ahead period forecast, `size` the expanding window size used for estimating the
model and `foreacast_date` the date the forecast corresponds to. The remaining column
headings are the forecasts of the conditional mean (`mu`) and volatility (`sigma`)
and any additional distributional parameters (which are fixed based on the estimated
model). Note that even if the model has no additional distributional parameters, the
defaults will still be printed since the (d,p,q,r) methods from the **tsdistribution**
package can take all this information, given some distribution, and will take care of
which distribution needs what. Finally, the `actual` column is the realized value of
y for each `forecast_date`.

Any interesting way to visualize the 1-step ahead forecasts is by looking at the value
at risk exceedances plot:

```{r,fig.width=7,fig.height=5}
library(tsdistributions)
h1 <- b$table[horizon == 1, .(filter_date, forecast_date, mu, sigma, skew, shape, lambda, actual)]
h1[,VaR := qdist(b$distribution, p = 0.025, mu = mu, sigma = sigma, skew = skew, shape = shape, lambda = lambda)]
main_title <-  paste("Daily Returns and Value-at-Risk \nExceedances (alpha=0.025)",sep = "")
par(mar = c(3,2,3,3))
plot(h1$forecast_date, h1$actual, type = "n", main = title, ylim =
         c(min(min(h1$actual), min(h1$VaR)),
           max(max(h1$actual), max(h1$VaR))),
     ann = FALSE, cex.lab = 0.9, cex.axis = 0.8)
title(main_title, cex.main = 0.8)
abline(h = 0, col = "grey", lty = 2)
points(h1$forecast_date, h1$actual, pch = 18, col = "snow3")
sel <- which(h1$actual < h1$VaR)
points(h1$forecast_date[sel],h1$actual[sel], pch = 18, col = "red")
lines(h1$forecast_date, h1$VaR, lwd = 2, col = "black")
legend("topleft", max(h1$actual), c("returns","return < VaR","VaR"),
       col = c("snow3", "red","black"), cex = 0.75,
       pch = c(18,18,-1), lty = c(0,0,1), lwd = c(0,0,2), bty = "n")
grid()
```



## Benchmark {#demo:benchmark}

The package has a built-in function which generates a table for the benchmark of [@Fiorentini1996], 
showing the log relative error between the values from **tsgarch** and the benchmark values,
and defined as:

$$
\textrm{LRE} = -\log_{10}\left(\frac{\left|x - b\right|}{\left|b\right|}\right)
$$
where $x$ is the estimated value and $b$ the benchmark value.

```{r}
benchmark <- benchmark_fcp()
as_flextable(benchmark)
```


# Concluding Remarks {#conclusion}

The package is still in active development. Additional models may be added, porting some of the remaining **rugarch** models, but the focus is likely
to be on models for high frequency data.

A separate package called [tstests](https://github.com/tsmodels/tstests) is also in the process of being developed which includes a number of tests 
which were previously part of **rugarch**.

Multivariate extensions such as those in [rmgarch](https://cran.r-project.org/web/packages/rmgarch/index.html) will be rolled 
out eventually in a separate package.

A blog site is available at [nopredict.com](https://www.nopredict.com/) where additional examples may
be posted in the future.

# Appendix {#appendix:functionmap}

```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(data.tree)
library(flextable)
x <- read.csv('function_map.csv')
colnames(x)[1] <- "(...)"
tsgarch_map <- as.Node(x)
map_df <- ToDataFrameTree(tsgarch_map, "type","input","output")
map_df[,1] <- gsub(" ", "\t", map_df[,1], fixed = TRUE)
names(map_df)[1] <- c("(...)")
map_df[1, 2:4] <- " "
out <- flextable(map_df) |> theme_alafoli() |> color(i = 1, j = 1, color = "purple")
out <- out |> color(i = 2, j = 1, color = "steelblue")
out <- out |> color(i = 7, j = 1, color = "steelblue")
out <- out |> color(i = 8, j = 1, color = "cadetblue")
out <- out |> color(i = 28, j = 1, color = "cadetblue")
out <- out |> color(i = 36, j = 1, color = "steelblue")
out <- out |> color(i = 37, j = 1, color = "steelblue")
out <- out |> set_caption(caption = "Table: tsgarch function map")
out <- out |> width(j = 1, width = 2)
out
```

# References


